```python
# %% [markdown]
# # ðŸš€ VisionFlow Real-Time Demo
# **Advanced Object Detection + Pose Estimation**  
# *Works on webcam/video files*

# %% [markdown]
# ## 1. Setup Environment
# Install required packages:

# %% [code]
!pip install -q ultralytics mediapipe opencv-python
!git clone https://github.com/yourusername/VisionFlow
%cd VisionFlow

# %% [markdown]
# ## 2. Initialize Models
# Load YOLOv8 (detection) and MediaPipe (pose):

# %% [code]
import cv2
from ultralytics import YOLO

# Initialize models
detector = YOLO("yolov8n.pt")
pose_estimator = mp.solutions.pose.Pose(
    static_image_mode=False,
    model_complexity=2,
    min_detection_confidence=0.7
)

# %% [markdown]
# ## 3. Real-Time Processing
# Run inference on webcam:

# %% [code]
cap = cv2.VideoCapture(0)  # 0 = default webcam

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break
    
    # Object detection
    detections = detector(frame)[0]
    annotated_frame = detections.plot()
    
    # Pose estimation
    results = pose_estimator.process(frame)
    if results.pose_landmarks:
        mp.solutions.drawing_utils.draw_landmarks(
            annotated_frame,
            results.pose_landmarks,
            mp.solutions.pose.POSE_CONNECTIONS
        )
    
    cv2.imshow('VisionFlow', annotated_frame)
    if cv2.waitKey(1) == ord('q'): break

cap.release()
cv2.destroyAllWindows()

# %% [markdown]
# ## 4. Sample Output
# ![Demo GIF](https://example.com/visionflow-demo.gif)  
# *Real-time detection + pose estimation*

# %% [markdown]
# ## 5. Next Steps
# - Try with custom videos: `cv2.VideoCapture("your_video.mp4")`  
# - Export to ONNX: `model.export(format="onnx")`  
# - Deploy on Jetson: See `scripts/deploy_edge.py`
```
